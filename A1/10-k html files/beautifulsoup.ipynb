{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: Import necessary libraries\n",
    "I have moved the iterations to the very end\n",
    "\"\"\"\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Prepare a function to clean the raw content, removing html tags and entities\n",
    "\"\"\"\n",
    "def clean_html(raw_html, remove_tags=True):\n",
    "    \"\"\" This function cleans the raw html content by removing html tags and entities\n",
    "    - remove_tags: this is an optional parameter,\n",
    "                if True, remove html tags, otherwise keep them\n",
    "                by default, it is set to True.\n",
    "    \"\"\"\n",
    "    if remove_tags:\n",
    "        # remove HTML tags\n",
    "        cleaned_html = re.sub(r'<.*?>', ' ', raw_html)\n",
    "        # remove HTML entities\n",
    "        cleaned_html = re.sub(r'&\\w+;', ' ', cleaned_html)\n",
    "        cleaned_html = re.sub(r\"&[a-z]+;\", \" \", cleaned_html)\n",
    "\n",
    "    else:\n",
    "        cleaned_html = re.sub(r'(<br\\s*/?>|</div>|</p>|</tr>|</li>|</table>|</td>)', '\\n', raw_html, flags=re.IGNORECASE)\n",
    "\n",
    "        cleaned_html = re.sub(r'<(?!/)[^>]+>', '', cleaned_html) # do not remove <\\span>\n",
    "        # cleaned_html = re.sub(r'<span>(?=\\w)', '', cleaned_html)  # Removes <span> if directly before a word character without a leading space\n",
    "        # cleaned_html = re.sub(r'(?<=\\w)</span>', '', cleaned_html)  # Removes </span> if directly after a word character without a trailing space\n",
    "\n",
    "\n",
    "    # replace HTML entities (&#160;）with space\n",
    "    cleaned_html = re.sub(r'&#\\d+;|nbsp', ' ', cleaned_html)\n",
    "    cleaned_html = re.sub(r'\\s+', ' ', cleaned_html)\n",
    "\n",
    "    # remove urls\n",
    "    cleaned_url = re.sub(r\"\\(http[s]?://\\S+\\)\", \"\", cleaned_html)\n",
    "    cleaned_url = re.sub(r\"http[s]?://\\S+\", \"\", cleaned_url)\n",
    "\n",
    "    # remove multiple spaces\n",
    "    cleaned_space = re.sub(r'\\s+', ' ', cleaned_url)\n",
    "\n",
    "    # fix the error that /s/ is splitted into / s / in GOOG_10-K_2021.html\n",
    "    clean_spe = re.sub(r'/\\s*S\\s*/', '/s/', cleaned_space, flags=re.IGNORECASE)\n",
    "\n",
    "    # fix the error: ALICE is splitted into multiple A LICE in MSFT_10-K_2021.html\n",
    "    clean_spe = re.sub(r'(\\b[A-Z])\\s([A-Z]{2,}\\b)', r'\\1\\2', clean_spe)\n",
    "\n",
    "    # fix the error: can not find the signature section in MSFT_10-K_2021.html\n",
    "    cleaned = re.sub(r'SIGNAT\\s*URES', 'SIGNATURES', clean_spe, flags=re.IGNORECASE)\n",
    "\n",
    "    return cleaned.strip()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Extract all necessary information from the cleaned text\n",
    "\"\"\"\n",
    "def convert_date(date_str):\n",
    "    \"\"\" This is a helper function that converts date to ISO format \"\"\"\n",
    "\n",
    "    try:\n",
    "        date = datetime.strptime(date_str.strip(), \"%B %d, %Y\").date().isoformat()\n",
    "    except ValueError:\n",
    "        date = re.sub(r\"\\s+,\", \",\", date_str) # e.g.: 'For the fiscal year ended December 31 , 2023'\n",
    "        date = datetime.strptime(date, \"%B %d, %Y\").date().isoformat()\n",
    "    return date\n",
    "\n",
    "\n",
    "def extract_fiscal_year(text):\n",
    "    \"\"\" 1. The date of the fiscal year-end (ensure it is formatted in ISO-format)\n",
    "\n",
    "    Currently, relevant information is in the beginning of the document, e.g.:\n",
    "     'For the fiscal year ended January 31, 2024, or'\n",
    "     Search for the fiscal year and return it in ISO-format.\n",
    "\n",
    "     \"\"\"\n",
    "    match = re.search(r\"for the fiscal year ended (\\w+\\s\\d{1,2}\\s*,\\s\\d{4})\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        fiscal_year_str = match.group(1)\n",
    "        fiscal_year_iso = convert_date(fiscal_year_str)\n",
    "        return fiscal_year_iso\n",
    "\n",
    "    return 'N/A'\n",
    "\n",
    "\n",
    "def extract_legal_proceedings(text):\n",
    "    \"\"\" 2. The content of “Item 3. LEGAL PROCEEDINGS”.\"\"\"\n",
    "\n",
    "    match = re.search(r\"Item 3\\. Legal Proceedings(.*?)(Item 4\\.|$)\", text, re.IGNORECASE|re.DOTALL)\n",
    "    return match.group(1).strip() if match else 'N/A'\n",
    "\n",
    "\n",
    "def extract_signature_date(text):\n",
    "    \"\"\" 3. The date of signature(s) (ensure it is formatted in ISO-format).\n",
    "\n",
    "    Find the SIGNATURES section\n",
    "\n",
    "    \"\"\"\n",
    "    signature_pattern = r\"SIGNATURES\\s*Pursuant to the requirements of Section.*?(?=EXHIBIT INDEX|$)\"\n",
    "    signature_section_match = re.search(signature_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    if signature_section_match:\n",
    "        signature_section = signature_section_match.group(0)\n",
    "        signature_date_match = re.search(r\"\\s*(\\w+\\s\\d{1,2}\\s*,\\s*\\d{4})\", signature_section)\n",
    "        # (?:Date:|as of | ) removed from the regex\n",
    "        if signature_date_match:\n",
    "            signature_date = convert_date(signature_date_match.group(1))\n",
    "            return signature_date\n",
    "    return 'N/A'\n",
    "\n",
    "\n",
    "def extract_signers(text):\n",
    "    \"\"\" 4. Who signed the report?\n",
    "    If there are multiple signatures, all of them have to be listed (comma separated).\n",
    "    Do not include the audit firm – in case it is given in the report.\"\"\"\n",
    "\n",
    "    positions = ['Chief Executive Officer', 'Chief Financial Officer', 'Chief Accounting Officer', 'President', 'Chairman', 'Director']\n",
    "\n",
    "    positions_str = ('Chief', 'Officer', 'President', 'Chairman', 'Director', 'Executive', 'Financial', 'Accounting', 'Manager')\n",
    "\n",
    "    # find the signature section\n",
    "    signature_pattern = r\"SIGNATURES(?:</span>)?\\sPursuant to the requirements of Section.*?(?=EXHIBIT INDEX|$)\"\n",
    "\n",
    "    signature_section_match = re.search(signature_pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    if not signature_section_match:\n",
    "        print('No signature section found')\n",
    "\n",
    "    if signature_section_match:\n",
    "        print('found')\n",
    "        signature_section = signature_section_match.group(0)\n",
    "\n",
    "        cleaned_signers = []\n",
    "        # TODO: 这个方法太粗暴了，如果四字人名就完蛋，最好的方法应该是写个新的clean text\n",
    "        # TODO： 确认一下到底是list还是set\n",
    "        # TODO: audit firm怎么去掉，hard-coded？\n",
    "        raw_signers = re.findall(r\"/s/\\s*([A-Z][a-zA-Z.\\-]+\\s[A-Z][a-zA-Z.\\-]+(?:\\s[A-Z][a-zA-Z.\\-]+)?)(?:</span>)?\", signature_section)\n",
    "\n",
    "        # TODO: GOOG和jnj都是找不到raw_signers，而msft和meta是连section都找不到\n",
    "        if not raw_signers:\n",
    "            print('section found, but no signers found')\n",
    "\n",
    "        for signer in raw_signers:\n",
    "            # 清理多余空格\n",
    "            signer = signer.strip()\n",
    "\n",
    "            # 去除职位\n",
    "            for position in positions_str:\n",
    "                if position in signer:\n",
    "                    signer = signer.replace(position, '')\n",
    "\n",
    "            # 检查是否有重复名字，例如 \"Cesar Conde Cesar\" -> \"Cesar Conde\"\n",
    "            words = signer.split()\n",
    "            if len(words) > 2 and words[-1].lower() == words[0].lower():  # 如果最后一个单词与第一个单词相同\n",
    "                signer = ' '.join(words[:-1])  # 去除最后一个重复单词\n",
    "\n",
    "            signer = signer.strip()\n",
    "            # 避免重复，保持顺序\n",
    "            if signer.lower() not in [s.lower() for s in cleaned_signers]:\n",
    "                cleaned_signers.append(signer)\n",
    "\n",
    "        return cleaned_signers\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def process_file(file):\n",
    "    clean_content_one = clean_html(file)\n",
    "    clean_content_two = clean_html(file, remove_tags=False)\n",
    "\n",
    "    fiscal_year = extract_fiscal_year(clean_content_one)\n",
    "    legal_proceedings = extract_legal_proceedings(clean_content_one)\n",
    "    signature_date = extract_signature_date(clean_content_one)\n",
    "    signers = extract_signers(clean_content_two)\n",
    "\n",
    "    if not signers:\n",
    "        signers = extract_signers(clean_content_one)\n",
    "    return {\n",
    "        \"fiscal_year\": fiscal_year,\n",
    "        \"legal_proceedings\": legal_proceedings,\n",
    "        \"signature_date\": signature_date,\n",
    "        \"signers\": signers\n",
    "    }\n",
    "\n",
    "current_directory = Path.cwd()\n",
    "results = []\n",
    "for html_file in current_directory.glob('*.html'):\n",
    "    with html_file.open('r', encoding='utf-8') as f:\n",
    "        filename = html_file.name\n",
    "        raw_content = f.read()\n",
    "        result = process_file(raw_content)\n",
    "        result['file_name'] = filename\n",
    "\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('10k_results.csv', index=False)\n",
    "df.head(10)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html_bs(raw_html, remove_tags=True):\n",
    "    \"\"\" This function cleans the raw HTML content using BeautifulSoup by removing html tags and entities.\n",
    "    - remove_tags: this is an optional parameter,\n",
    "                if True, remove html tags, otherwise keep them\n",
    "                by default, it is set to True.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "    if remove_tags:\n",
    "        # Remove all script and style elements\n",
    "        for script_or_style in soup([\"script\", \"style\"]):\n",
    "            script_or_style.decompose()\n",
    "\n",
    "        # Get text\n",
    "        text = soup.get_text(separator=' ')\n",
    "    else:\n",
    "        # If not removing tags, just clean up entities and unnecessary whitespace\n",
    "        text = str(soup)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Example usage in your Step 3 to extract information\n",
    "def process_file_with_bs(file):\n",
    "    clean_content_one = clean_html_bs(file)\n",
    "    clean_content_two = clean_html_bs(file, remove_tags=False)\n",
    "\n",
    "    fiscal_year = extract_fiscal_year(clean_content_one)\n",
    "    legal_proceedings = extract_legal_proceedings(clean_content_one)\n",
    "    signature_date = extract_signature_date(clean_content_one)\n",
    "    signers = extract_signers(clean_content_two)\n",
    "\n",
    "    if not signers:\n",
    "        signers = extract_signers(clean_content_one)\n",
    "    return {\n",
    "        \"fiscal_year\": fiscal_year,\n",
    "        \"legal_proceedings\": legal_proceedings,\n",
    "        \"signature_date\": signature_date,\n",
    "        \"signers\": signers\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
